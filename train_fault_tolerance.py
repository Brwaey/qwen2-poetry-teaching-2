import torch
import torch.nn as nn
import torch.optim as optim
import time
import numpy as np
import pickle
from tqdm import tqdm
from dataset_fault_tolerance import create_fault_tolerance_dataset, get_fault_tolerance_dataloader

# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"å¾®è°ƒè®¾å¤‡ï¼š{device}ï¼ˆGPUåŠ é€Ÿå·²å¯ç”¨ï¼‰" if torch.cuda.is_available() else "å¾®è°ƒè®¾å¤‡ï¼šCPUï¼ˆå»ºè®®ä½¿ç”¨GPUæå‡é€Ÿåº¦ï¼‰")

def load_pretrained_model(model_path, vocab, embed_dim=256, num_heads=4, num_layers=3, max_seq_len=64):
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç”¨äºå¾®è°ƒ"""
    from train import ExactMatchModel
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = ExactMatchModel(
        vocab_size=len(vocab),
        embed_dim=embed_dim,
        num_heads=num_heads,
        num_layers=num_layers,
        max_seq_len=max_seq_len
    ).to(device)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æˆåŠŸï¼š{model_path}")
        return model
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·å…ˆè¿è¡ŒåŸå§‹train.pyè®­ç»ƒåŸºç¡€æ¨¡å‹")
        return None

def fine_tune_fault_tolerance(pretrained_model_path="exact_match_poem_model.pth",
                             max_train_time=600,  # å¾®è°ƒæ—¶é—´10åˆ†é’Ÿ
                             batch_size=32):
    """å¾®è°ƒæ¨¡å‹ä»¥å¢å¼ºå®¹é”™èƒ½åŠ›"""
    # 1. åŠ è½½è¯æ±‡è¡¨
    try:
        with open("exact_match_vocab.pkl", "rb") as f:
            vocab = pickle.load(f)
        print(f"âœ… åŠ è½½è¯æ±‡è¡¨æˆåŠŸï¼š{len(vocab)}ä¸ªæ ‡è®°")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°è¯æ±‡è¡¨æ–‡ä»¶ exact_match_vocab.pkl")
        print("è¯·å…ˆè¿è¡ŒåŸå§‹train.pyç”Ÿæˆè¯æ±‡è¡¨")
        return None
    
    # 2. åˆ›å»ºå®¹é”™è¾“å…¥æ•°æ®é›†
    dataset, vocab = create_fault_tolerance_dataset(
        original_poems_path="original_poems.pkl",
        vocab=vocab,
        total_samples=10000,
        max_length=64
    )
    if dataset is None:
        return None
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = get_fault_tolerance_dataloader(
        dataset,
        batch_size=batch_size,
        shuffle=True
    )
    print(f"æ•°æ®åŠ è½½å™¨é…ç½®ï¼šæ‰¹æ¬¡å¤§å°={batch_size}ï¼Œæ€»æ‰¹æ¬¡æ•°={len(dataloader)}")
    
    # 4. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = load_pretrained_model(pretrained_model_path, vocab)
    if model is None:
        return None
    
    # 5. å¾®è°ƒä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼‰
    # åªå¾®è°ƒæœ€åå‡ å±‚ï¼Œå†»ç»“å‰é¢çš„å±‚
    for param in list(model.parameters())[:-10]:  # å†»ç»“å¤§éƒ¨åˆ†å‚æ•°
        param.requires_grad = False
    
    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),  # åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
        lr=5e-5,  # å¾®è°ƒå­¦ä¹ ç‡è¦å°ï¼Œé¿å…ç ´åé¢„è®­ç»ƒçŸ¥è¯†
        weight_decay=1e-6
    )
    
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=len(dataloader) * 4,
        eta_min=1e-6
    )
    
    criterion = nn.CrossEntropyLoss(
        ignore_index=0  # å¿½ç•¥PAD
    )
    
    # 6. å¾®è°ƒå‚æ•°
    train_losses = []
    best_loss = float("inf")
    no_improve_count = 0
    start_time = time.time()
    best_model_state = None
    loss_improve_threshold = 0.002  # å¾®è°ƒçš„æ”¹è¿›é˜ˆå€¼æ›´å°
    
    # 7. å¼€å§‹å¾®è°ƒ
    print(f"\nå®¹é”™è¾“å…¥å¾®è°ƒå¯åŠ¨ï¼ˆé™æ—¶{max_train_time//60}åˆ†é’Ÿï¼‰...")
    model.train()
    for epoch in range(20):  # å¾®è°ƒè½®æ¬¡è¾ƒå°‘
        elapsed_time = time.time() - start_time
        if elapsed_time >= max_train_time:
            print(f"\nâœ… è¾¾åˆ°å¾®è°ƒæ—¶é—´é™åˆ¶ï¼ˆ{max_train_time}ç§’ï¼‰ï¼Œåœæ­¢è®­ç»ƒ")
            break
        
        epoch_loss = 0.0
        progress_bar = tqdm(
            dataloader,
            desc=f"Epoch {epoch+1:2d}/20 | è€—æ—¶ï¼š{elapsed_time:.0f}ç§’ | æœ€ä½³æŸå¤±ï¼š{best_loss:.4f}"
        )
        
        for batch_idx, (inputs, labels) in enumerate(progress_bar):
            inputs = inputs.to(device, non_blocking=torch.cuda.is_available())
            labels = labels.to(device, non_blocking=torch.cuda.is_available())
            
            optimizer.zero_grad()
            logits = model(inputs)
            loss = criterion(
                logits.reshape(-1, logits.size(-1)),
                labels.view(-1)
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.3)  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
            optimizer.step()
            scheduler.step()
            
            epoch_loss += loss.item()
            avg_batch_loss = epoch_loss / (batch_idx + 1)
            progress_bar.set_postfix({
                "batch_loss": f"{loss.item():.4f}",
                "avg_loss": f"{avg_batch_loss:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.6f}"
            })
        
        avg_epoch_loss = epoch_loss / len(dataloader)
        train_losses.append(avg_epoch_loss)
        print(f"ğŸ“Š Epoch {epoch+1:2d} ç»“æŸ | å¹³å‡æŸå¤±ï¼š{avg_epoch_loss:.4f} | è€—æ—¶ï¼š{elapsed_time:.0f}ç§’")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_epoch_loss < best_loss - loss_improve_threshold:
            best_loss = avg_epoch_loss
            best_model_state = model.state_dict().copy()
            no_improve_count = 0
            print(f"ğŸ”§ æœ€ä½³æ¨¡å‹æ›´æ–°ï¼šå½“å‰æŸå¤±{avg_epoch_loss:.4f} < å†å²æœ€ä½³{best_loss+loss_improve_threshold:.4f}")
        else:
            no_improve_count += 1
            # æ—©åœæ¡ä»¶æ›´å®½æ¾
            if no_improve_count >= 5:
                print(f"\nâœ… æŸå¤±è¿ç»­5æ¬¡æ— æ”¹å–„ï¼Œåœæ­¢å¾®è°ƒ")
                break
    
    # 8. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
    if best_model_state is not None:
        torch.save(best_model_state, "fault_tolerance_poem_model.pth")
        print(f"\nğŸ“ å®¹é”™è¾“å…¥æ¨¡å‹å·²ä¿å­˜ï¼šfault_tolerance_poem_model.pthï¼ˆæœ€ä½³æŸå¤±ï¼š{best_loss:.4f}ï¼‰")
    else:
        torch.save(model.state_dict(), "fault_tolerance_poem_model.pth")
        print(f"\nğŸ“ å®¹é”™è¾“å…¥æ¨¡å‹å·²ä¿å­˜ï¼šfault_tolerance_poem_model.pthï¼ˆå½“å‰æŸå¤±ï¼š{avg_epoch_loss:.4f}ï¼‰")
    
    # 9. ä¿å­˜æŸå¤±æ›²çº¿
    with open("fault_tolerance_losses.pkl", "wb") as f:
        pickle.dump(train_losses, f)
    
    # 10. ç»˜åˆ¶æŸå¤±æ›²çº¿
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(10, 4))
        plt.plot(train_losses, color="#2ECC71", linewidth=2, marker="o", markersize=4)
        plt.title("å®¹é”™è¾“å…¥æ¨¡å‹å¾®è°ƒæŸå¤±æ›²çº¿", fontsize=12)
        plt.xlabel("å¾®è°ƒè½®æ¬¡ï¼ˆEpochï¼‰", fontsize=10)
        plt.ylabel("å¹³å‡æŸå¤±å€¼", fontsize=10)
        plt.grid(alpha=0.3, linestyle="--")
        plt.savefig("fault_tolerance_loss_curve.png", dpi=100, bbox_inches="tight")
        print(f"ğŸ“Š æŸå¤±æ›²çº¿å·²ä¿å­˜ï¼šfault_tolerance_loss_curve.png")
    except ImportError:
        print("âš ï¸  æœªå®‰è£…matplotlibï¼Œè·³è¿‡æŸå¤±æ›²çº¿ç»˜åˆ¶")
    
    return model, vocab, train_losses

if __name__ == "__main__":
    model, vocab, losses = fine_tune_fault_tolerance()
    print("\nğŸ‰ å®¹é”™è¾“å…¥æ¨¡å‹å¾®è°ƒå®Œæˆï¼å¯è¿è¡Œ inference_fault_tolerance.py ä½“éªŒå¸¦å®¹é”™èƒ½åŠ›çš„è¯—è¯ç”Ÿæˆã€‚")
    