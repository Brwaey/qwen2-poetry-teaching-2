import torch
import pickle
import editdistance  # ç”¨äºè®¡ç®—å­—ç¬¦ä¸²ç¼–è¾‘è·ç¦»
from tqdm import tqdm

# ç‰¹æ®Šæ ‡è®°é…ç½®ï¼ˆä¸è®­ç»ƒå®Œå…¨ä¸€è‡´ï¼‰
PAD_TOKEN = "<pad>"
UNK_TOKEN = "<unk>"
BOS_TOKEN = "<bos>"
EOS_TOKEN = "<eos>"
PAD_ID = 0
UNK_ID = 1
BOS_ID = 2
EOS_ID = 3

# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ç”Ÿæˆè®¾å¤‡ï¼š{device}")

def load_fault_tolerance_resources(
    model_path="fault_tolerance_poem_model.pth",
    vocab_path="exact_match_vocab.pkl",
    poems_path="original_poems.pkl"
):
    """åŠ è½½å®¹é”™è¾“å…¥æ¨¡å‹æ‰€éœ€èµ„æº"""
    # 1. åŠ è½½åŸè¯—åº“
    try:
        with open(poems_path, "rb") as f:
            original_poems = pickle.load(f)
        print(f"âœ… åŸè¯—åº“åŠ è½½æˆåŠŸï¼šå…±{len(original_poems)}é¦–è¯—ï¼ˆå«æ ‡å‡†å¥å¼ï¼‰")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°åŸè¯—åº“æ–‡ä»¶ {poems_path}")
        return None, None, None
    
    # 2. åŠ è½½è¯æ±‡è¡¨ï¼ˆä¿®å¤å±æ€§åæ‹¼å†™é”™è¯¯ï¼‰
    try:
        with open(vocab_path, "rb") as f:
            vocab = pickle.load(f)
        # ä¿®å¤ï¼šoriginal_poems_chars â†’ original_poem_chars
        print(f"âœ… åŸè¯—è¯æ±‡è¡¨åŠ è½½æˆåŠŸï¼š{len(vocab)}ä¸ªæ ‡è®°ï¼ˆå«{len(vocab.original_poem_chars)}ä¸ªåŸè¯—å•å­—ï¼‰")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°è¯æ±‡è¡¨æ–‡ä»¶ {vocab_path}")
        return None, None, original_poems
    
    # 3. åŠ è½½æ¨¡å‹
    from train import ExactMatchModel
    model = ExactMatchModel(
        vocab_size=len(vocab),
        embed_dim=256,
        num_heads=4,
        num_layers=3,
        max_seq_len=64
    ).to(device)
    
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        print(f"âœ… å®¹é”™è¾“å…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼š{model_path}")
        return model, vocab, original_poems
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·å…ˆè¿è¡Œ train_fault_tolerance.py è®­ç»ƒæ¨¡å‹")
        return None, vocab, original_poems
    

def find_best_match(input_text, original_poems, max_edit_distance=2):
    """
    å¯»æ‰¾ä¸è¾“å…¥æ–‡æœ¬æœ€åŒ¹é…çš„åŸè¯—å¼€å¤´ï¼ˆå®¹å¿ä¸€å®šé”™è¯¯ï¼‰
    ä½¿ç”¨ç¼–è¾‘è·ç¦»è¡¡é‡å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
    """
    input_text_clean = input_text.replace("ï¼Œ", "").replace("ã€‚", "").strip()
    if len(input_text_clean) == 0:
        return None, 0, 0
    
    best_match_poem = None
    best_start_idx = -1
    min_edit_dist = float('inf')
    best_match_text = ""
    
    for poem in original_poems:
        # æ£€æŸ¥è¯—ä¸­çš„æ‰€æœ‰å¯èƒ½å¼€å¤´
        for start_text in poem["all_starts"]:
            # è®¡ç®—ç¼–è¾‘è·ç¦»ï¼ˆè¡¡é‡å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼‰
            dist = editdistance.eval(input_text_clean, start_text)
            
            # ä¼˜å…ˆé€‰æ‹©é•¿åº¦ç›¸è¿‘ä¸”ç¼–è¾‘è·ç¦»å°çš„åŒ¹é…
            length_diff = abs(len(input_text_clean) - len(start_text))
            combined_score = dist + length_diff * 0.5  # ç»¼åˆè¯„åˆ†
            
            # æ›´æ–°æœ€ä½³åŒ¹é…
            if combined_score < min_edit_dist and dist <= max_edit_distance:
                min_edit_dist = combined_score
                best_match_poem = poem
                best_match_text = start_text
                # æ‰¾åˆ°åœ¨å®Œæ•´æ— æ ‡ç‚¹æ–‡æœ¬ä¸­çš„ä½ç½®
                best_start_idx = poem["full_text_no_punc"].find(start_text)
    
    if best_match_poem is not None:
        print(f"âœ… å®¹é”™åŒ¹é…ï¼šè¾“å…¥ '{input_text}' åŒ¹é…åˆ°åŸè¯—å¼€å¤´ '{best_match_text}'ï¼ˆç¼–è¾‘è·ç¦»ï¼š{min_edit_dist}ï¼‰")
        return best_match_poem, best_start_idx, min_edit_dist
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„åŸè¯—å¼€å¤´ï¼ˆæœ€å¤§å…è®¸ç¼–è¾‘è·ç¦»ï¼š{max_edit_distance}ï¼‰")
        return None, 0, 0

def add_standard_punctuation(text_no_punc, poem_pattern):
    """æŒ‰åŸè¯—æ ‡å‡†å¥å¼æ·»åŠ æ ‡ç‚¹"""
    char_per_sentence = poem_pattern["char_per_sentence"]
    punctuation_pos = poem_pattern["punctuation_pos"]
    punctuation_type = poem_pattern["punctuation_type"]
    
    text_with_punc = ""
    char_count = 0
    
    for char in text_no_punc:
        text_with_punc += char
        char_count += 1
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æ ‡ç‚¹ä½ç½®
        if char_count in punctuation_pos:
            pos_idx = punctuation_pos.index(char_count)
            if pos_idx < len(punctuation_type):
                text_with_punc += punctuation_type[pos_idx]
    
    # ç¡®ä¿æœ€åä¸€å¥æœ‰å¥å·
    if text_with_punc and text_with_punc[-1] not in ["ã€‚", "ï¼Œ"]:
        text_with_punc += "ã€‚"
    
    # ç§»é™¤è¿ç»­æ ‡ç‚¹
    text_with_punc = text_with_punc.replace("ï¼Œï¼Œ", "ï¼Œ").replace("ã€‚ã€‚", "ã€‚").replace("ï¼Œã€‚", "ã€‚")
    return text_with_punc

def generate_with_fault_tolerance(
    model, vocab, original_poems, input_text,
    max_length=64,
    max_edit_distance=2  # æœ€å¤§å…è®¸ç¼–è¾‘è·ç¦»
):
    """å¸¦å®¹é”™èƒ½åŠ›çš„è¯—è¯ç”Ÿæˆ"""
    try:
        # 1. å®¹é”™åŒ¹é…åŸè¯—
        matched_poem, start_idx, edit_dist = find_best_match(
            input_text, original_poems, max_edit_distance
        )
        if matched_poem is None:
            return "âŒ æœªæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„åŸè¯—ï¼Œæ— æ³•ç”Ÿæˆ"
        
        input_text_clean = input_text.replace("ï¼Œ", "").replace("ã€‚", "").strip()
        full_poem_no_punc = matched_poem["full_text_no_punc"]
        poem_pattern = matched_poem["sentence_pattern"]
        target_total_length = len(full_poem_no_punc)
        
        # 2. è¾“å…¥é¢„å¤„ç†ï¼ˆä½¿ç”¨åŒ¹é…åˆ°çš„æ­£ç¡®å¼€å¤´è¿›è¡Œå¤„ç†ï¼‰
        correct_start = matched_poem["all_starts"][0]  # å–åŒ¹é…åˆ°çš„è¯—çš„ç¬¬ä¸€ä¸ªæ­£ç¡®å¼€å¤´
        for start in matched_poem["all_starts"]:
            if full_poem_no_punc.find(start) == start_idx:
                correct_start = start
                break
                
        input_tokens = [BOS_TOKEN] + vocab.tokenize(input_text_clean)
        input_ids = vocab.convert_tokens_to_ids(input_tokens)
        
        # 3. åˆå§‹åŒ–ç”Ÿæˆåºåˆ—
        generated_ids = torch.tensor([input_ids], dtype=torch.long).to(device)
        
        # 4. é€è¯ç”Ÿæˆ
        with torch.no_grad():
            required_steps = target_total_length - len(input_text_clean)
            for _ in range(required_steps):
                seq_len = generated_ids.shape[1]
                if seq_len >= max_length - 1:
                    break
                
                # æ¨¡å‹é¢„æµ‹
                logits = model(generated_ids)
                next_token_logits = logits[:, -1, :]
                
                # ä»…ä¿ç•™åŸè¯—ä¸­çš„å­—ä½œä¸ºå€™é€‰
                original_char_ids = [vocab.token_to_id[char] for char in full_poem_no_punc 
                                    if char in vocab.token_to_id]
                
                # æ’é™¤éåŸè¯—å­—
                for token_id in range(len(vocab)):
                    if token_id not in original_char_ids and token_id not in [PAD_ID, UNK_ID, BOS_ID, EOS_ID]:
                        next_token_logits[0, token_id] = -1e9
                
                # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å­—
                next_token_id = torch.argmax(next_token_logits, dim=-1).item()
                
                # ç»ˆæ­¢æ¡ä»¶
                current_generated_no_punc = "".join([
                    vocab.id_to_token[id.item()] if isinstance(id, torch.Tensor) else vocab.id_to_token[id]
                    for id in generated_ids[0]
                    if id not in [PAD_ID, UNK_ID, BOS_ID, EOS_ID]
                ])
                
                if current_generated_no_punc == full_poem_no_punc or next_token_id == EOS_ID:
                    break
                
                # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
                generated_ids = torch.cat([
                    generated_ids,
                    torch.tensor([[next_token_id]], dtype=torch.long).to(device)
                ], dim=1)
        
        # 5. åå¤„ç†
        generated_no_punc = "".join([
            vocab.id_to_token[id.item()] if isinstance(id, torch.Tensor) else vocab.id_to_token[id]
            for id in generated_ids[0]
            if id not in [PAD_ID, UNK_ID, BOS_ID, EOS_ID]
        ])
        
        # è¡¥å……åŸè¯—ç¼ºå¤±éƒ¨åˆ†
        if len(generated_no_punc) < len(full_poem_no_punc):
            generated_no_punc = full_poem_no_punc
        
        # æ·»åŠ æ ‡ç‚¹
        final_poem = add_standard_punctuation(generated_no_punc, poem_pattern)
        
        # è¿˜åŸè¾“å…¥æ—¶çš„æ ‡ç‚¹
        if input_text.endswith("ï¼Œ") and not final_poem.startswith(input_text):
            input_punc_pos = len(input_text_clean)
            if input_punc_pos < len(final_poem):
                final_poem = final_poem[:input_punc_pos] + "ï¼Œ" + final_poem[input_punc_pos:]
        
        return final_poem
    except Exception as e:
        import traceback
        print(f"ç”Ÿæˆé”™è¯¯è¯¦æƒ…ï¼š{traceback.format_exc()}")
        return f"âŒ ç”Ÿæˆé”™è¯¯ï¼š{str(e)}"

def interactive_fault_tolerance_demo():
    """å®¹é”™è¾“å…¥äº¤äº’å¼æ¼”ç¤º"""
    print("=" * 80)
    print("          ğŸ“œ Qwen2ä¸­æ–‡è¯—è¯ç”Ÿæˆï¼ˆå®¹é”™è¾“å…¥å¢å¼ºç‰ˆï¼‰          ")
    print("=" * 80)
    print("ğŸ“š æ”¯æŒå¸¦é”™åˆ«å­—ã€æ¼å­—çš„è¯—å¥å¼€å¤´ï¼ˆå®¹é”™è¾“å…¥ï¼‰ï¼Œä¾‹å¦‚ï¼š")
    print("  - è¾“å…¥ 'åºŠå‰æ˜æœˆå¹¿' â†’ åº”åŒ¹é… 'åºŠå‰æ˜æœˆå…‰' å¹¶ç”Ÿæˆå®Œæ•´ã€Šé™å¤œæ€ã€‹")
    print("  - è¾“å…¥ 'ç™½æ—¥ä¾å±±' â†’ åº”åŒ¹é… 'ç™½æ—¥ä¾å±±å°½' å¹¶ç”Ÿæˆå®Œæ•´ã€Šç™»é¹³é›€æ¥¼ã€‹")
    print("  - è¾“å…¥ 'å›½ç ´å±±æ²³å†' â†’ åº”åŒ¹é… 'å›½ç ´å±±æ²³åœ¨' å¹¶ç”Ÿæˆå®Œæ•´ã€Šæ˜¥æœ›ã€‹")
    print("ğŸ’¡ è¾“å…¥ 'exit' é€€å‡ºæ¼”ç¤ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
    print("=" * 80)
    
    # åŠ è½½æ‰€æœ‰èµ„æº
    print("\næ­£åœ¨åŠ è½½å®¹é”™è¾“å…¥æ¨¡å‹èµ„æº...")
    model, vocab, original_poems = load_fault_tolerance_resources()
    if model is None or vocab is None or original_poems is None:
        print("âŒ æ¼”ç¤ºå¯åŠ¨å¤±è´¥ï¼Œè¯·ç¡®ä¿æ‰€æœ‰èµ„æºæ–‡ä»¶å·²ç”Ÿæˆ")
        return
    
    # äº¤äº’å¾ªç¯
    print("\nâœ… æ¼”ç¤ºå¯åŠ¨æˆåŠŸï¼è¯·è¾“å…¥å¸¦å¯èƒ½é”™è¯¯çš„è¯—å¥å¼€å¤´ï¼ˆå¦‚'åºŠå‰æ˜æœˆå¹¿'ï¼‰ï¼š")
    while True:
        try:
            user_input = input("\nä½ è¾“å…¥çš„è¯—å¥å¼€å¤´ï¼š").strip()
            
            # å‘½ä»¤å¤„ç†
            if user_input.lower() == "exit":
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå®¹é”™è¾“å…¥è¯—è¯ç”Ÿæˆæ¼”ç¤ºç»“æŸï¼")
                break
            elif user_input.lower() == "help":
                print("ğŸ“– å¸®åŠ©è¯´æ˜ï¼š")
                print("  1. å¯ä»¥è¾“å…¥å¸¦é”™åˆ«å­—çš„å¼€å¤´ï¼ˆå¦‚'åºŠå‰æ˜æœˆå¹¿'ï¼‰")
                print("  2. å¯ä»¥è¾“å…¥ä¸å®Œæ•´çš„å¼€å¤´ï¼ˆå¦‚'ç™½æ—¥ä¾å±±'ï¼‰")
                print("  3. å¯ä»¥è¾“å…¥å¸¦å¤šä½™å­—çš„å¼€å¤´ï¼ˆå¦‚'åºŠå‰çš„æ˜æœˆå…‰'ï¼‰")
                print("  4. è¾“å…¥ 'exit' é€€å‡ºæ¼”ç¤ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
                continue
            
            # ç”Ÿæˆè¯—è¯
            print("âœï¸  æ­£åœ¨å®¹é”™åŒ¹é…å¹¶ç”ŸæˆåŸè¯—...")
            result = generate_with_fault_tolerance(model, vocab, original_poems, user_input)
            
            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ¯ å®¹é”™ç”Ÿæˆç»“æœï¼š")
            matched_poem, _, _ = find_best_match(user_input, original_poems)
            if matched_poem:
                print(f"ã€Š{matched_poem['title']}ã€‹")
            print(result)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ‰‹åŠ¨ä¸­æ–­ï¼Œæ¼”ç¤ºç»“æŸï¼")
            break
        except Exception as e:
            print(f"\nâŒ æ“ä½œé”™è¯¯ï¼š{str(e)}ï¼Œè¯·é‡è¯•")

if __name__ == "__main__":
    interactive_fault_tolerance_demo()
    