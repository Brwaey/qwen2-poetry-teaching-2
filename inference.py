import torch
import pickle
import random
from tqdm import tqdm

# ç‰¹æ®Šæ ‡è®°é…ç½®ï¼ˆä¸è®­ç»ƒå®Œå…¨ä¸€è‡´ï¼‰
PAD_TOKEN = "<pad>"
UNK_TOKEN = "<unk>"
BOS_TOKEN = "<bos>"
EOS_TOKEN = "<eos>"
PAD_ID = 0
UNK_ID = 1
BOS_ID = 2
EOS_ID = 3

# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ç”Ÿæˆè®¾å¤‡ï¼š{device}")

def load_exact_match_resources(
    model_path="exact_match_poem_model.pth",
    vocab_path="exact_match_vocab.pkl",
    poems_path="original_poems.pkl"
):
    """åŠ è½½åŸè¯—ç²¾å‡†åŒ¹é…æ‰€éœ€èµ„æºï¼šæ¨¡å‹ã€è¯æ±‡è¡¨ã€åŸè¯—åº“ï¼ˆå«æ ‡å‡†å¥å¼ï¼‰"""
    # 1. åŠ è½½åŸè¯—åº“ï¼ˆå…³é”®ï¼šç”¨äºåŒ¹é…è¾“å…¥å¼€å¤´å¯¹åº”çš„åŸè¯—å’Œå¥å¼ï¼‰
    try:
        with open(poems_path, "rb") as f:
            original_poems = pickle.load(f)
        print(f"âœ… åŸè¯—åº“åŠ è½½æˆåŠŸï¼šå…±{len(original_poems)}é¦–è¯—ï¼ˆå«æ ‡å‡†å¥å¼ï¼‰")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°åŸè¯—åº“æ–‡ä»¶ {poems_path}")
        print("è¯·å…ˆè¿è¡Œ train.py ç”ŸæˆåŸè¯—åº“")
        return None, None, None
    
    # 2. åŠ è½½è¯æ±‡è¡¨
    try:
        with open(vocab_path, "rb") as f:
            vocab = pickle.load(f)
        print(f"âœ… åŸè¯—è¯æ±‡è¡¨åŠ è½½æˆåŠŸï¼š{len(vocab)}ä¸ªæ ‡è®°ï¼ˆå«{len(vocab.original_poem_chars)}ä¸ªåŸè¯—å•å­—ï¼‰")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°è¯æ±‡è¡¨æ–‡ä»¶ {vocab_path}")
        print("è¯·å…ˆè¿è¡Œ train.py ç”Ÿæˆè¯æ±‡è¡¨")
        return None, None, original_poems
    
    # 3. åŠ è½½æ¨¡å‹
    from train import ExactMatchModel  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
    model = ExactMatchModel(
        vocab_size=len(vocab),
        embed_dim=256,
        num_heads=4,
        num_layers=3,
        max_seq_len=64
    ).to(device)
    
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()  # æ¨ç†æ¨¡å¼ï¼ˆç¦ç”¨Dropoutï¼‰
        print(f"âœ… åŸè¯—ç²¾å‡†åŒ¹é…æ¨¡å‹åŠ è½½æˆåŠŸï¼š{model_path}")
        return model, vocab, original_poems
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·å…ˆè¿è¡Œ train.py è®­ç»ƒæ¨¡å‹")
        return None, vocab, original_poems

def match_input_to_poem(input_text, original_poems):
    """
    æ ¸å¿ƒåŠŸèƒ½ï¼šæ ¹æ®è¾“å…¥å¼€å¤´åŒ¹é…å¯¹åº”çš„åŸè¯—
    è¿”å›ï¼šåŒ¹é…åˆ°çš„åŸè¯—ä¿¡æ¯ï¼ˆå«æ ‡å‡†å¥å¼ï¼‰ã€è¾“å…¥åœ¨åŸè¯—ä¸­çš„èµ·å§‹ä½ç½®
    """
    input_text_clean = input_text.replace("ï¼Œ", "").replace("ã€‚", "").strip()
    if len(input_text_clean) == 0:
        return None, 0
    
    # éå†åŸè¯—åº“ï¼Œæ‰¾åˆ°åŒ…å«è¯¥å¼€å¤´çš„åŸè¯—
    matched_poem = None
    start_idx_in_poem = -1
    max_match_len = 0  # è®°å½•æœ€é•¿åŒ¹é…é•¿åº¦ï¼ˆç¡®ä¿åŒ¹é…æœ€ç²¾å‡†çš„åŸè¯—ï¼‰
    
    for poem in original_poems:
        full_text_no_punc = poem["full_text_no_punc"]
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦æ˜¯å½“å‰è¯—çš„å¼€å¤´
        if input_text_clean in full_text_no_punc:
            current_start_idx = full_text_no_punc.find(input_text_clean)
            current_match_len = len(input_text_clean)
            # é€‰æ‹©æœ€é•¿åŒ¹é…çš„åŸè¯—ï¼ˆé¿å…çŸ­å¼€å¤´åŒ¹é…é”™è¯¯ï¼‰
            if current_match_len > max_match_len:
                max_match_len = current_match_len
                start_idx_in_poem = current_start_idx
                matched_poem = poem
    
    if matched_poem is None:
        print(f"âš ï¸  æœªåŒ¹é…åˆ°åŸè¯—ï¼Œè¯·è¾“å…¥ä»¥ä¸‹åŸè¯—çš„å¼€å¤´ï¼š")
        for poem in original_poems:
            print(f"- ã€Š{poem['title']}ã€‹ï¼š{poem['content']}")
        return None, 0
    
    print(f"âœ… åŒ¹é…åˆ°åŸè¯—ï¼šã€Š{matched_poem['title']}ã€‹")
    return matched_poem, start_idx_in_poem

def add_standard_punctuation(text_no_punc, poem_pattern):
    """æŒ‰åŸè¯—æ ‡å‡†å¥å¼æ·»åŠ æ ‡ç‚¹ï¼ˆæ— ä»»ä½•éšæœºï¼Œç¡®ä¿æ ‡ç‚¹æ­£ç¡®ï¼‰"""
    char_per_sentence = poem_pattern["char_per_sentence"]
    punctuation_pos = poem_pattern["punctuation_pos"]
    punctuation_type = poem_pattern["punctuation_type"]
    
    text_with_punc = ""
    char_count = 0
    
    for char in text_no_punc:
        text_with_punc += char
        char_count += 1
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æ ‡ç‚¹ä½ç½®
        if char_count in punctuation_pos:
            # æ‰¾åˆ°å¯¹åº”ä½ç½®çš„æ ‡ç‚¹ç±»å‹
            pos_idx = punctuation_pos.index(char_count)
            if pos_idx < len(punctuation_type):
                text_with_punc += punctuation_type[pos_idx]
    
    # ç¡®ä¿æœ€åä¸€å¥æœ‰å¥å·ï¼ˆè‹¥ç¼ºå¤±ï¼‰
    if text_with_punc and text_with_punc[-1] not in ["ã€‚", "ï¼Œ"]:
        text_with_punc += "ã€‚"
    
    # ç§»é™¤è¿ç»­æ ‡ç‚¹ï¼ˆåŒé‡ä¿é™©ï¼‰
    text_with_punc = text_with_punc.replace("ï¼Œï¼Œ", "ï¼Œ").replace("ã€‚ã€‚", "ã€‚").replace("ï¼Œã€‚", "ã€‚")
    return text_with_punc

def generate_exact_poem(
    model, vocab, original_poems, input_text,
    max_length=64  # è¶³å¤Ÿå®¹çº³æœ€é•¿åŸè¯—ï¼ˆå¾‹è¯—56å­—ï¼‰
):
    """
    åŸè¯—ç²¾å‡†ç”Ÿæˆï¼š
    1. å…ˆåŒ¹é…è¾“å…¥å¯¹åº”çš„åŸè¯—
    2. æŒ‰åŸè¯—æ ‡å‡†å¥å¼ç”Ÿæˆå†…å®¹å’Œæ ‡ç‚¹
    3. ç¦æ­¢è·¨è¯—å†…å®¹ï¼Œç¡®ä¿100%è´´åˆåŸè¯—
    """
    try:
        # 1. åŒ¹é…è¾“å…¥å¯¹åº”çš„åŸè¯—
        matched_poem, start_idx = match_input_to_poem(input_text, original_poems)
        if matched_poem is None:
            return "âŒ æœªåŒ¹é…åˆ°åŸè¯—ï¼Œæ— æ³•ç”Ÿæˆ"
        
        input_text_clean = input_text.replace("ï¼Œ", "").replace("ã€‚", "").strip()
        full_poem_no_punc = matched_poem["full_text_no_punc"]
        poem_pattern = matched_poem["sentence_pattern"]
        target_total_length = len(full_poem_no_punc)  # ç›®æ ‡é•¿åº¦ï¼šå®Œæ•´åŸè¯—é•¿åº¦
        
        # 2. è¾“å…¥é¢„å¤„ç†ï¼ˆå•å­—åˆ†è¯ï¼‰
        input_tokens = [BOS_TOKEN] + vocab.tokenize(input_text_clean)
        input_ids = vocab.convert_tokens_to_ids(input_tokens)
        
        # 3. åˆå§‹åŒ–ç”Ÿæˆåºåˆ—
        generated_ids = torch.tensor([input_ids], dtype=torch.long).to(device)
        generated_tokens = input_tokens.copy()
        
        # 4. é€è¯ç”Ÿæˆï¼ˆä¸¥æ ¼è´´åˆåŸè¯—ï¼‰
        with torch.no_grad():
            # ç”Ÿæˆåˆ°å®Œæ•´åŸè¯—é•¿åº¦å³å¯ï¼ˆæ— éœ€è¿‡é•¿ï¼‰
            required_steps = target_total_length - len(input_text_clean)
            for _ in range(required_steps):
                seq_len = generated_ids.shape[1]
                if seq_len >= max_length - 1:
                    break
                
                # æ¨¡å‹é¢„æµ‹
                logits = model(generated_ids)
                next_token_logits = logits[:, -1, :]
                
                # å…³é”®ï¼šä»…ä¿ç•™åŸè¯—ä¸­çš„å­—ä½œä¸ºå€™é€‰ï¼ˆæœç»è·¨è¯—å†…å®¹ï¼‰
                original_char_ids = []
                for char in full_poem_no_punc:
                    if char in vocab.token_to_id:
                        original_char_ids.append(vocab.token_to_id[char])
                
                # å°†éåŸè¯—å­—çš„æ¦‚ç‡è®¾ä¸º-æ— ç©·ï¼ˆä¸è¢«é€‰æ‹©ï¼‰
                for token_id in range(len(vocab)):  # ä¿®å¤ï¼šä½¿ç”¨len(vocab)è€Œévocab.vocab_size
                    if token_id not in original_char_ids and token_id not in [PAD_ID, UNK_ID, BOS_ID, EOS_ID]:
                        next_token_logits[0, token_id] = -1e9
                
                # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å­—ï¼ˆæ— éšæœºï¼Œç¡®ä¿åŸè¯—åŒ¹é…ï¼‰
                next_token_id = torch.argmax(next_token_logits, dim=-1).item()  # ç¡®ä¿è½¬æ¢ä¸ºæ ‡é‡
                
                # ç»ˆæ­¢æ¡ä»¶ï¼šç”Ÿæˆåˆ°å®Œæ•´åŸè¯—é•¿åº¦ï¼Œæˆ–é‡åˆ°EOS
                current_generated_no_punc = "".join([
                    vocab.id_to_token[id.item()] if isinstance(id, torch.Tensor) else vocab.id_to_token[id]
                    for id in generated_ids[0]
                    if id not in [PAD_ID, UNK_ID, BOS_ID, EOS_ID]
                ])
                
                if current_generated_no_punc == full_poem_no_punc or next_token_id == EOS_ID:
                    break
                
                # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
                generated_ids = torch.cat([
                    generated_ids,
                    torch.tensor([[next_token_id]], dtype=torch.long).to(device)
                ], dim=1)
                generated_tokens.append(vocab.id_to_token[next_token_id])
        
        # 5. åå¤„ç†ï¼šç”Ÿæˆå®Œæ•´åŸè¯—æ–‡æœ¬
        # æå–ç”Ÿæˆçš„æ— æ ‡ç‚¹æ–‡æœ¬
        generated_no_punc = "".join([
            vocab.id_to_token[id.item()] if isinstance(id, torch.Tensor) else vocab.id_to_token[id]
            for id in generated_ids[0]
            if id not in [PAD_ID, UNK_ID, BOS_ID, EOS_ID]
        ])
        
        # è¡¥å……åŸè¯—ç¼ºå¤±éƒ¨åˆ†ï¼ˆè‹¥ç”Ÿæˆä¸å®Œæ•´ï¼‰
        if len(generated_no_punc) < len(full_poem_no_punc):
            generated_no_punc = full_poem_no_punc  # ç›´æ¥ä½¿ç”¨åŸè¯—æ— æ ‡ç‚¹æ–‡æœ¬ï¼Œç¡®ä¿å®Œæ•´
        
        # æŒ‰åŸè¯—æ ‡å‡†å¥å¼æ·»åŠ æ ‡ç‚¹
        final_poem = add_standard_punctuation(generated_no_punc, poem_pattern)
        
        # è¿˜åŸè¾“å…¥æ—¶çš„æ ‡ç‚¹ï¼ˆå¦‚è¾“å…¥"åºŠå‰æ˜æœˆå…‰ï¼Œ"ï¼Œä¿ç•™é€—å·ï¼‰
        if input_text.endswith("ï¼Œ") and not final_poem.startswith(input_text):
            input_punc_pos = len(input_text_clean)
            if input_punc_pos < len(final_poem):
                final_poem = final_poem[:input_punc_pos] + "ï¼Œ" + final_poem[input_punc_pos:]
        
        return final_poem
    except Exception as e:
        # è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•
        import traceback
        print(f"ç”Ÿæˆé”™è¯¯è¯¦æƒ…ï¼š{traceback.format_exc()}")
        return f"âŒ ç”Ÿæˆé”™è¯¯ï¼š{str(e)}"

def interactive_exact_demo():
    """åŸè¯—ç²¾å‡†åŒ¹é…äº¤äº’å¼æ¼”ç¤º"""
    print("=" * 75)
    print("          ğŸ“œ Qwen2ä¸­æ–‡è¯—è¯ç”Ÿæˆï¼ˆåŸè¯—100%ç²¾å‡†åŒ¹é…ç‰ˆï¼‰          ")
    print("=" * 75)
    print("ğŸ“š æ”¯æŒä»¥ä¸‹åŸè¯—çš„ç²¾å‡†ç»­å†™ï¼ˆè¾“å…¥å¼€å¤´å³å¯ï¼‰ï¼š")
    # åŠ è½½åŸè¯—åº“ç”¨äºæ˜¾ç¤ºæ”¯æŒçš„è¯—æ­Œ
    try:
        with open("original_poems.pkl", "rb") as f:
            original_poems = pickle.load(f)
        for i, poem in enumerate(original_poems, 1):
            print(f"  {i}. ã€Š{poem['title']}ã€‹ï¼š{poem['content']}")
    except:
        pass
    print("ğŸ’¡ è¾“å…¥ 'exit' é€€å‡ºæ¼”ç¤ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
    print("=" * 75)
    
    # åŠ è½½æ‰€æœ‰èµ„æº
    print("\næ­£åœ¨åŠ è½½åŸè¯—ç²¾å‡†åŒ¹é…èµ„æº...")
    model, vocab, original_poems = load_exact_match_resources()
    if model is None or vocab is None or original_poems is None:
        print("âŒ æ¼”ç¤ºå¯åŠ¨å¤±è´¥ï¼Œè¯·ç¡®ä¿æ‰€æœ‰èµ„æºæ–‡ä»¶å·²ç”Ÿæˆ")
        return
    
    # äº¤äº’å¾ªç¯
    print("\nâœ… æ¼”ç¤ºå¯åŠ¨æˆåŠŸï¼è¯·è¾“å…¥ä¸Šè¿°åŸè¯—çš„å¼€å¤´ï¼ˆå¦‚'åºŠå‰æ˜æœˆå…‰'ï¼‰ï¼š")
    while True:
        try:
            user_input = input("\nä½ è¾“å…¥çš„è¯—å¥å¼€å¤´ï¼š").strip()
            
            # å‘½ä»¤å¤„ç†
            if user_input.lower() == "exit":
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼ŒåŸè¯—ç²¾å‡†åŒ¹é…æ¼”ç¤ºç»“æŸï¼")
                break
            elif user_input.lower() == "help":
                print("ğŸ“– å¸®åŠ©è¯´æ˜ï¼š")
                print("  1. è¾“å…¥ä¸Šè¿°æ”¯æŒåŸè¯—çš„å¼€å¤´ï¼ˆ1-10å­—ï¼Œå¦‚'åºŠå‰æ˜æœˆå…‰'ã€'å›½ç ´å±±æ²³åœ¨'ï¼‰")
                print("  2. æ”¯æŒå¸¦æ ‡ç‚¹è¾“å…¥ï¼ˆå¦‚'åºŠå‰æ˜æœˆå…‰ï¼Œ'ï¼‰ï¼Œç”Ÿæˆç»“æœä¼šä¿ç•™æ ‡ç‚¹æ ¼å¼")
                print("  3. è¾“å…¥ 'exit' é€€å‡ºæ¼”ç¤ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
                continue
            
            # ç”Ÿæˆè¯—è¯
            print("âœï¸  æ­£åœ¨ç”ŸæˆåŸè¯—...")
            result = generate_exact_poem(model, vocab, original_poems, user_input)
            
            # æ˜¾ç¤ºç»“æœï¼ˆçªå‡ºæ˜¾ç¤ºåŸè¯—æ ‡é¢˜ï¼‰
            print(f"\nğŸ¯ åŸè¯—ç²¾å‡†åŒ¹é…ç»“æœï¼š")
            # åŒ¹é…åŸè¯—æ ‡é¢˜
            matched_poem, _ = match_input_to_poem(user_input, original_poems)
            if matched_poem:
                print(f"ã€Š{matched_poem['title']}ã€‹")
            print(result)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ‰‹åŠ¨ä¸­æ–­ï¼Œæ¼”ç¤ºç»“æŸï¼")
            break
        except Exception as e:
            print(f"\nâŒ æ“ä½œé”™è¯¯ï¼š{str(e)}ï¼Œè¯·é‡è¯•")

if __name__ == "__main__":
    interactive_exact_demo()
    