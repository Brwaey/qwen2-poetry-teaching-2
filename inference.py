import torch
import jieba
import pickle
import random
from tqdm import tqdm
from train import EnhancedQwen2PoetryModel
from dataset import EnhancedVocab

# ç‰¹æ®Šæ ‡è®°é…ç½®ï¼ˆä¸è®­ç»ƒå®Œå…¨ä¸€è‡´ï¼‰
PAD_TOKEN = "<pad>"
UNK_TOKEN = "<unk>"
BOS_TOKEN = "<bos>"
EOS_TOKEN = "<eos>"
PAD_ID = 0    # PAD_TOKENçš„ID
UNK_ID = 1    # UNK_TOKENçš„ID
BOS_ID = 2    # BOS_TOKENçš„ID
EOS_ID = 3    # EOS_TOKENçš„ID

# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ç”Ÿæˆè®¾å¤‡ï¼š{device}")

def load_model_and_vocab(
    model_path="qwen2_best_poetry_model.pth",  # åŠ è½½æœ€ä½³æ¨¡å‹
    vocab_path="vocab_enhanced.pkl"
):
    """åŠ è½½å¢å¼ºç‰ˆæ¨¡å‹å’Œè¯æ±‡è¡¨"""
    # 1. åŠ è½½è¯æ±‡è¡¨
    try:
        with open(vocab_path, "rb") as f:
            vocab = pickle.load(f)
        print(f"âœ… å¢å¼ºç‰ˆè¯æ±‡è¡¨åŠ è½½æˆåŠŸï¼š{len(vocab)}ä¸ªæ ‡è®°")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°è¯æ±‡è¡¨æ–‡ä»¶ {vocab_path}")
        print("è¯·å…ˆè¿è¡Œ train.py ç”Ÿæˆè¯æ±‡è¡¨")
        return None, None
    
    # 2. åŠ è½½å¢å¼ºç‰ˆæ¨¡å‹
    model = EnhancedQwen2PoetryModel(
        vocab_size=len(vocab),
        embed_dim=256,
        num_heads=4,
        num_layers=4,
        max_seq_len=64
    ).to(device)
    
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()  # æ¨ç†æ¨¡å¼
        print(f"âœ… å¢å¼ºç‰ˆæ¨¡å‹åŠ è½½æˆåŠŸï¼š{model_path}")
        return model, vocab
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·å…ˆè¿è¡Œ train.py è®­ç»ƒæ¨¡å‹")
        return None, vocab

def find_cutoff_index(cumulative_probs, top_p):
    """å…¼å®¹ä½ç‰ˆæœ¬PyTorchçš„Top-Pæˆªæ–­é€»è¾‘"""
    cum_probs_np = cumulative_probs.cpu().numpy()[0]
    cutoff_idx = len(cum_probs_np)
    for i in range(len(cum_probs_np)):
        if cum_probs_np[i] >= top_p:
            cutoff_idx = i
            break
    return max(1, cutoff_idx)

def generate_poem(
    model, vocab, input_text,
    max_length=48,        # å¢åŠ ç”Ÿæˆé•¿åº¦ï¼ˆ32â†’48ï¼‰
    temperature=0.25,     # é™ä½æ¸©åº¦ï¼ˆ0.35â†’0.25ï¼‰ï¼Œæ›´ç¡®å®š
    top_k=8,              # å¢åŠ å€™é€‰è¯ï¼ˆ6â†’8ï¼‰
    top_p=0.95,           # æé«˜ç´¯ç§¯æ¦‚ç‡ï¼ˆ0.92â†’0.95ï¼‰
    repeat_penalty=1.2,   # é™ä½æƒ©ç½šåŠ›åº¦ï¼ˆ1.5â†’1.2ï¼‰ï¼Œé¿å…è¿‡æ—©åœ
    max_repeat=3          # å…è®¸æ›´å¤šé‡å¤ï¼ˆ2â†’3ï¼‰
):
    """ä¼˜åŒ–ç”Ÿæˆé€»è¾‘ï¼šç¡®ä¿ç”Ÿæˆå®Œæ•´ã€é€šé¡ºçš„è¯—å¥"""
    if model is None or vocab is None:
        return "âŒ æ¨¡å‹æˆ–è¯æ±‡è¡¨æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆè¯—è¯"
    
    # 1. è¾“å…¥é¢„å¤„ç†
    input_tokens = [BOS_TOKEN] + vocab.tokenize(input_text)
    input_ids = vocab.convert_tokens_to_ids(input_tokens)
    
    # è¾“å…¥é•¿åº¦æ£€æŸ¥
    if len(input_ids) >= max_length:
        return "âŒ è¾“å…¥è¿‡é•¿ï¼Œè¯·æ§åˆ¶åœ¨10å­—ä»¥å†…"
    if len(input_text.strip()) == 0:
        return "âŒ è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·è¾“å…¥è¯—å¥å¼€å¤´"
    
    # 2. åˆå§‹åŒ–ç”Ÿæˆåºåˆ—
    generated_ids = torch.tensor([input_ids], dtype=torch.long).to(device)
    generated_tokens = input_tokens.copy()
    
    # 3. é€è¯ç”Ÿæˆè¯—è¯ï¼ˆå¢åŠ æœ€å°ç”Ÿæˆé•¿åº¦ï¼‰
    min_generate_length = max(8, 16 - len(input_ids))  # è‡³å°‘ç”Ÿæˆ8-16å­—
    with torch.no_grad():
        for step in range(max_length - len(input_ids)):
            seq_len = generated_ids.shape[1]
            
            # æ¨¡å‹é¢„æµ‹
            logits = model(generated_ids)
            next_token_logits = logits[:, -1, :]
            
            # 4. æ”¹è¿›çš„é‡å¤æƒ©ç½šï¼ˆåŒºåˆ†åŠŸèƒ½è¯å’Œå†…å®¹è¯ï¼‰
            for token_id in generated_ids[0]:
                token = vocab.id_to_token.get(token_id, "")
                # å¯¹å†…å®¹è¯æƒ©ç½šé‡ï¼ŒåŠŸèƒ½è¯ï¼ˆå¦‚"ï¼Œ"ï¼‰æƒ©ç½šè½»
                if token_id not in [PAD_ID, UNK_ID, BOS_ID, EOS_ID] and len(token) > 0:
                    penalty = repeat_penalty if len(token) > 1 else 1.05
                    next_token_logits[0, token_id] /= penalty
            
            # 5. æ¸©åº¦è°ƒæ•´
            next_token_logits = next_token_logits / temperature
            
            # 6. Top-Pæ ¸é‡‡æ ·
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            probs = torch.softmax(sorted_logits, dim=-1)
            cumulative_probs = probs.cumsum(dim=-1)
            
            cutoff_idx = find_cutoff_index(cumulative_probs, top_p)
            sorted_logits = sorted_logits[:, :cutoff_idx+1]
            sorted_indices = sorted_indices[:, :cutoff_idx+1]
            
            # 7. Top-Kè¿‡æ»¤
            if top_k is not None and top_k < len(sorted_indices[0]):
                sorted_logits = sorted_logits[:, :top_k]
                sorted_indices = sorted_indices[:, :top_k]
            
            # 8. é‡æ–°è®¡ç®—æ¦‚ç‡
            probs = torch.softmax(sorted_logits, dim=-1)
            next_token_idx = torch.multinomial(probs, num_samples=1).item()
            next_token_id = sorted_indices[0, next_token_idx].item()
            
            # 9. æ”¹è¿›çš„é‡å¤æ£€æŸ¥ï¼ˆå…è®¸åˆç†é‡å¤ï¼‰
            if len(generated_ids[0]) >= max_repeat:
                last_tokens = generated_ids[0][-max_repeat:].tolist()
                if all(token == next_token_id for token in last_tokens):
                    if len(sorted_indices[0]) > 1:
                        next_token_id = sorted_indices[0, 1].item()
                    else:
                        common_tokens = [
                            tid for tid in range(len(vocab))
                            if tid not in generated_ids[0] and tid not in [PAD_ID, UNK_ID, BOS_ID, EOS_ID]
                        ]
                        if common_tokens:
                            next_token_id = random.choice(common_tokens)
            
            # 10. æ”¹è¿›çš„åœæ­¢æ¡ä»¶ï¼ˆç¡®ä¿ç”Ÿæˆè¶³å¤Ÿé•¿åº¦ï¼‰
            if next_token_id == EOS_ID:
                # æœªè¾¾åˆ°æœ€å°é•¿åº¦åˆ™å¿½ç•¥EOS
                if step < min_generate_length:
                    # é€‰æ‹©æ¦‚ç‡ç¬¬äºŒé«˜çš„è¯
                    if len(sorted_indices[0]) > 1:
                        next_token_id = sorted_indices[0, 1].item()
                    else:
                        next_token_id = random.choice([tid for tid in range(len(vocab)) 
                                                     if tid not in [PAD_ID, UNK_ID, BOS_ID, EOS_ID]])
                else:
                    break  # è¾¾åˆ°æœ€å°é•¿åº¦åˆ™åœæ­¢
            
            # 11. æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
            generated_ids = torch.cat([
                generated_ids,
                torch.tensor([[next_token_id]], dtype=torch.long).to(device)
            ], dim=1)
            generated_tokens.append(vocab.id_to_token[next_token_id])
    
    # 12. æ”¹è¿›çš„åå¤„ç†ï¼ˆæ›´æ™ºèƒ½çš„æ ‡ç‚¹æ·»åŠ ï¼‰
    final_tokens = [
        token for token in generated_tokens
        if token not in [BOS_TOKEN, EOS_TOKEN, PAD_TOKEN, UNK_TOKEN]
    ]
    final_text = "".join(final_tokens)
    
    # æå–ç»­å†™éƒ¨åˆ†
    ç»­å†™_text = final_text[len(input_text):] if len(final_text) >= len(input_text) else ""
    full_text = input_text + ç»­å†™_text
    
    # æ™ºèƒ½æ ‡ç‚¹æ·»åŠ ï¼ˆåŸºäºè¯—å¥ç»“æ„ï¼‰
    punctuated_text = ""
    char_count = 0
    for i, char in enumerate(full_text):
        # è·³è¿‡å·²æœ‰çš„æ ‡ç‚¹
        if char in ["ï¼Œ", "ã€‚", "ï¼", "ï¼Ÿ"]:
            punctuated_text += char
            continue
            
        punctuated_text += char
        char_count += 1
        
        # 7å­—ä¸€å¥ï¼ˆå”è¯—å¸¸è§æ ¼å¼ï¼‰
        if char_count % 7 == 0 and i != len(full_text) - 1:
            # æ£€æŸ¥ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¦å·²ç»æ˜¯æ ‡ç‚¹
            if i + 1 < len(full_text) and full_text[i+1] not in ["ï¼Œ", "ã€‚"]:
                punctuated_text += "ï¼Œ"
        # 14å­—ä¸€ç»“ï¼ˆä¸¤å¥ä¸€ç»“ï¼‰
        elif char_count % 14 == 0 and i != len(full_text) - 1:
            if i + 1 < len(full_text) and full_text[i+1] not in ["ï¼Œ", "ã€‚"]:
                punctuated_text += "ã€‚"
    
    # ç¡®ä¿æœ€åæœ‰å¥å·
    if len(punctuated_text) > 0 and punctuated_text[-1] not in ["ã€‚", "ï¼", "ï¼Ÿ"]:
        punctuated_text += "ã€‚"
    
    return punctuated_text

def interactive_demo():
    """äº¤äº’å¼è¯—è¯ç”Ÿæˆæ¼”ç¤º"""
    print("=" * 60)
    print("          ğŸ“œ å¢å¼ºç‰ˆåŸºäºQwen2çš„ä¸­æ–‡è¯—è¯ç”Ÿæˆæ•™å­¦æ¼”ç¤º          ")
    print("=" * 60)
    print("ğŸ“š æ”¯æŒå”è¯—å®‹è¯ç»­å†™ï¼Œç¤ºä¾‹è¾“å…¥ï¼š")
    print("  - åºŠå‰æ˜æœˆå…‰ â†’ ç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚")
    print("  - ç™½æ—¥ä¾å±±å°½ â†’ é»„æ²³å…¥æµ·æµã€‚æ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚")
    print("  - å›½ç ´å±±æ²³åœ¨ â†’ åŸæ˜¥è‰æœ¨æ·±ã€‚æ„Ÿæ—¶èŠ±æº…æ³ªï¼Œæ¨åˆ«é¸ŸæƒŠå¿ƒã€‚")
    print("ğŸ’¡ è¾“å…¥ 'exit' é€€å‡ºæ¼”ç¤ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œè¯æ±‡è¡¨
    print("\næ­£åœ¨åŠ è½½å¢å¼ºç‰ˆæ¨¡å‹å’Œè¯æ±‡è¡¨...")
    model, vocab = load_model_and_vocab()
    if model is None:
        print("âŒ æ¼”ç¤ºå¯åŠ¨å¤±è´¥ï¼Œè¯·å…ˆç¡®ä¿æ¨¡å‹å’Œè¯æ±‡è¡¨å·²ç”Ÿæˆ")
        return
    
    # äº¤äº’å¾ªç¯
    print("\nâœ… æ¼”ç¤ºå¯åŠ¨æˆåŠŸï¼è¯·è¾“å…¥è¯—å¥å¼€å¤´ï¼š")
    while True:
        try:
            user_input = input("\nä½ è¾“å…¥çš„è¯—å¥å¼€å¤´ï¼š").strip()
            
            # å‘½ä»¤å¤„ç†
            if user_input.lower() == "exit":
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œè¯—è¯ç”Ÿæˆæ¼”ç¤ºç»“æŸï¼")
                break
            elif user_input.lower() == "help":
                print("ğŸ“– å¸®åŠ©è¯´æ˜ï¼š")
                print("  1. è¾“å…¥è¯—å¥å¼€å¤´ï¼ˆ1-10å­—ï¼‰ï¼Œæ¨¡å‹å°†ç»­å†™å®Œæ•´è¯—è¯")
                print("  2. è¾“å…¥ 'exit' é€€å‡ºæ¼”ç¤ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
                print("  3. ç¤ºä¾‹ï¼šè¾“å…¥'åºŠå‰æ˜æœˆå…‰'ï¼Œç”Ÿæˆå®Œæ•´ã€Šé™å¤œæ€ã€‹")
                continue
            
            # ç”Ÿæˆè¯—è¯
            print("âœï¸  æ­£åœ¨ç”Ÿæˆè¯—è¯...")
            result = generate_poem(model, vocab, user_input)
            
            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ¯ ç”Ÿæˆç»“æœï¼š{result}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ‰‹åŠ¨ä¸­æ–­ï¼Œæ¼”ç¤ºç»“æŸï¼")
            break
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆé”™è¯¯ï¼š{str(e)}ï¼Œè¯·é‡è¯•")

if __name__ == "__main__":
    interactive_demo()
