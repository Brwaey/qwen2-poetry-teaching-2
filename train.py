import torch
import torch.nn as nn
import torch.optim as optim
import time
import numpy as np
import pickle
from tqdm import tqdm
from dataset import create_exact_match_dataset, get_exact_match_dataloader

# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"è®­ç»ƒè®¾å¤‡ï¼š{device}ï¼ˆGPUåŠ é€Ÿå·²å¯ç”¨ï¼‰" if torch.cuda.is_available() else "è®­ç»ƒè®¾å¤‡ï¼šCPUï¼ˆå»ºè®®ä½¿ç”¨GPUæå‡é€Ÿåº¦ï¼‰")

class ExactMatchModel(nn.Module):
    """åŸè¯—ç²¾å‡†åŒ¹é…æ¨¡å‹ï¼šç®€åŒ–ç»“æ„ä½†å¼ºåŒ–æ‹Ÿåˆèƒ½åŠ›ï¼Œç¡®ä¿åŸè¯—è®°å¿†"""
    def __init__(self, vocab_size, embed_dim=256, num_heads=4, num_layers=3, max_seq_len=64):
        super().__init__()
        self.embed_dim = embed_dim
        self.max_seq_len = max_seq_len
        self.vocab_size = vocab_size
        
        # è¯åµŒå…¥å±‚ï¼šåŸè¯—å•å­—å°‘ï¼Œç”¨è¾ƒå¤§åµŒå…¥ç»´åº¦æå‡åŒºåˆ†åº¦
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embed_dim,
            padding_idx=0
        )
        
        # ä½ç½®ç¼–ç ï¼šä¸¥æ ¼é€‚é…åŸè¯—å¥å¼ï¼ˆ5/7å­—æ¯å¥ï¼‰
        self.pos_encoding = self._create_poem_positional_encoding(embed_dim, max_seq_len)
        self.pos_encoding.requires_grad = False
        
        # Transformer Encoderï¼š3å±‚è¶³å¤Ÿæ‹ŸåˆåŸè¯—ï¼ˆé¿å…è¿‡å¤æ‚å¯¼è‡´æ³›åŒ–é”™è¯¯ï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=1024,
            dropout=0.05,  # æä½dropoutï¼Œç¡®ä¿åŸè¯—ä¿¡æ¯ä¸ä¸¢å¤±
            layer_norm_eps=1e-5,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=num_layers
        )
        
        # è¾“å‡ºå±‚ï¼šæ— æ¿€æ´»å‡½æ•°ï¼Œç›´æ¥æ˜ å°„ï¼ˆç¡®ä¿åŸè¯—å­—çš„æ¦‚ç‡æœ€å¤§åŒ–ï¼‰
        self.final_norm = nn.LayerNorm(embed_dim)
        self.output_proj = nn.Linear(embed_dim, vocab_size)

    def _create_poem_positional_encoding(self, embed_dim, max_seq_len):
        """ä½ç½®ç¼–ç ï¼šé€‚é…åŸè¯—å¥å¼ï¼ˆæ¯5/7å­—ä¸€ä¸ªå‘¨æœŸï¼Œå¢å¼ºå¥å¼è®°å¿†ï¼‰"""
        position = torch.arange(max_seq_len, dtype=torch.float32).unsqueeze(1)
        # å‘¨æœŸè®¾ç½®ä¸º5å’Œ7çš„æœ€å°å…¬å€æ•°35ï¼Œé€‚é…ä¸¤ç§å¥å¼
        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float32) * (-np.log(35.0) / embed_dim))
        pos_encoding = torch.zeros(max_seq_len, embed_dim, dtype=torch.float32)
        
        # æ­£å¼¦ä½™å¼¦äº¤æ›¿ï¼Œå‘¨æœŸ35ï¼Œè´´åˆåŸè¯—å¥å¼
        pos_encoding[:, 0::2] = torch.sin(position * div_term)
        pos_encoding[:, 1::2] = torch.cos(position * div_term)
        pos_encoding = pos_encoding.unsqueeze(0)
        return pos_encoding

    def forward(self, x):
        batch_size, seq_len = x.shape
        x_embed = self.embedding(x)
        x_embed = x_embed * np.sqrt(self.embed_dim)  # å½’ä¸€åŒ–
        pos_encoding = self.pos_encoding[:, :seq_len, :].to(x.device)
        x_embed = x_embed + pos_encoding
        
        encoder_out = self.transformer_encoder(x_embed)
        encoder_out = self.final_norm(encoder_out)
        logits = self.output_proj(encoder_out)
        
        return logits

def train_exact_match_model(max_train_time=1200):  # å»¶é•¿è®­ç»ƒåˆ°20åˆ†é’Ÿï¼Œç¡®ä¿å……åˆ†æ‹Ÿåˆ
    """
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. å»é™¤æ ‡ç­¾å¹³æ»‘ï¼Œè®©æ¨¡å‹ç²¾å‡†å­¦ä¹ åŸè¯—æ ‡ç­¾
    2. ä½å­¦ä¹ ç‡+æ…¢é€€ç«ï¼Œç¡®ä¿æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜
    3. æ—©åœé˜ˆå€¼0.4ï¼Œç¡®ä¿æŸå¤±è¶³å¤Ÿä½ï¼ˆåŸè¯—æ‹Ÿåˆå……åˆ†ï¼‰
    """
    # 1. åŠ è½½åŸè¯—ç²¾å‡†åŒ¹é…æ•°æ®é›†
    dataset, vocab = create_exact_match_dataset(
        total_samples=20000,
        max_length=64
    )
    
    # 2. ä¿å­˜è¯æ±‡è¡¨ï¼ˆä¸æ¨¡å‹ä¸¥æ ¼å¯¹åº”ï¼‰
    with open("exact_match_vocab.pkl", "wb") as f:
        pickle.dump(vocab, f)
    print(f"åŸè¯—åŒ¹é…è¯æ±‡è¡¨å·²ä¿å­˜ï¼šexact_match_vocab.pklï¼ˆå¤§å°ï¼š{len(vocab)}ï¼‰")
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆCPUæ‰¹æ¬¡32ï¼ŒGPUæ‰¹æ¬¡64ï¼‰
    batch_size = 64 if torch.cuda.is_available() else 32
    dataloader = get_exact_match_dataloader(
        dataset,
        batch_size=batch_size,
        shuffle=True
    )
    print(f"æ•°æ®åŠ è½½å™¨é…ç½®ï¼šæ‰¹æ¬¡å¤§å°={batch_size}ï¼Œæ€»æ‰¹æ¬¡æ•°={len(dataloader)}")
    
    # 4. åˆå§‹åŒ–åŸè¯—åŒ¹é…æ¨¡å‹
    model = ExactMatchModel(
        vocab_size=len(vocab),
        embed_dim=256,
        num_heads=4,
        num_layers=3,
        max_seq_len=64
    ).to(device)
    print("åŸè¯—ç²¾å‡†åŒ¹é…æ¨¡å‹ç»“æ„ï¼š3å±‚Transformer Encoder + 256ç»´åµŒå…¥ï¼ˆå¼ºåŒ–åŸè¯—æ‹Ÿåˆï¼‰")
    
    # 5. ä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•°ï¼ˆæ— æ ‡ç­¾å¹³æ»‘ï¼Œç²¾å‡†æ‹Ÿåˆï¼‰
    optimizer = optim.AdamW(
        model.parameters(),
        lr=3e-4,  # ä½å­¦ä¹ ç‡ï¼Œç¼“æ…¢æ”¶æ•›
        weight_decay=1e-6,  # æä½æƒé‡è¡°å‡ï¼Œä¿æŠ¤åŸè¯—å‚æ•°
        betas=(0.9, 0.99)  # å¤§åŠ¨é‡ï¼Œç¨³å®šæ”¶æ•›
    )
    
    # ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼šå‘¨æœŸé•¿ï¼Œç¡®ä¿å……åˆ†æ”¶æ•›
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=len(dataloader) * 8,  # æ¯8ä¸ªepoché€€ç«ä¸€æ¬¡
        eta_min=5e-6  # æœ€å°å­¦ä¹ ç‡ï¼ŒåæœŸç²¾ç»†è°ƒæ•´
    )
    
    # æŸå¤±å‡½æ•°ï¼šæ— æ ‡ç­¾å¹³æ»‘ï¼Œç¡®ä¿åŸè¯—æ ‡ç­¾ç²¾å‡†
    criterion = nn.CrossEntropyLoss(
        ignore_index=0  # ä»…å¿½ç•¥PADï¼Œå…¶ä»–æ ‡ç­¾ä¸¥æ ¼åŒ¹é…
    )
    
    # 6. è®­ç»ƒå‚æ•°ï¼ˆä¸¥æ ¼æ—©åœï¼‰
    train_losses = []
    best_loss = float("inf")
    no_improve_count = 0
    start_time = time.time()
    best_model_state = None
    loss_improve_threshold = 0.005  # æŸå¤±é™ä½0.005ä»¥ä¸Šæ‰ç®—æ”¹å–„ï¼Œé¿å…æ³¢åŠ¨
    
    # 7. å¼€å§‹è®­ç»ƒï¼ˆå¼ºåŒ–åŸè¯—æ‹Ÿåˆï¼‰
    print(f"\nè®­ç»ƒå¯åŠ¨ï¼ˆé™æ—¶{max_train_time//60}åˆ†é’Ÿï¼Œç›®æ ‡æŸå¤±<0.4ï¼‰...")
    model.train()
    for epoch in range(60):  # æœ€å¤§60ä¸ªepochï¼Œè¶³å¤Ÿæ‹ŸåˆåŸè¯—
        elapsed_time = time.time() - start_time
        if elapsed_time >= max_train_time:
            print(f"\nâœ… è¾¾åˆ°è®­ç»ƒæ—¶é—´é™åˆ¶ï¼ˆ{max_train_time}ç§’ï¼‰ï¼Œåœæ­¢è®­ç»ƒ")
            break
        
        epoch_loss = 0.0
        progress_bar = tqdm(
            dataloader,
            desc=f"Epoch {epoch+1:2d}/60 | è€—æ—¶ï¼š{elapsed_time:.0f}ç§’ | æœ€ä½³æŸå¤±ï¼š{best_loss:.4f}"
        )
        
        for batch_idx, (inputs, labels) in enumerate(progress_bar):
            inputs = inputs.to(device, non_blocking=torch.cuda.is_available())
            labels = labels.to(device, non_blocking=torch.cuda.is_available())
            
            optimizer.zero_grad()
            logits = model(inputs)
            loss = criterion(
                logits.reshape(-1, logits.size(-1)),
                labels.view(-1)
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # ä½æ¢¯åº¦è£å‰ªï¼Œé¿å…å‚æ•°éœ‡è¡
            optimizer.step()
            scheduler.step()
            
            epoch_loss += loss.item()
            avg_batch_loss = epoch_loss / (batch_idx + 1)
            progress_bar.set_postfix({
                "batch_loss": f"{loss.item():.4f}",
                "avg_loss": f"{avg_batch_loss:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.6f}"
            })
        
        avg_epoch_loss = epoch_loss / len(dataloader)
        train_losses.append(avg_epoch_loss)
        print(f"ğŸ“Š Epoch {epoch+1:2d} ç»“æŸ | å¹³å‡æŸå¤±ï¼š{avg_epoch_loss:.4f} | è€—æ—¶ï¼š{elapsed_time:.0f}ç§’")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæŸå¤±é™ä½è¶³å¤Ÿå¤šï¼‰
        if avg_epoch_loss < best_loss - loss_improve_threshold:
            best_loss = avg_epoch_loss
            best_model_state = model.state_dict().copy()
            no_improve_count = 0
            print(f"ğŸ”§ æœ€ä½³æ¨¡å‹æ›´æ–°ï¼šå½“å‰æŸå¤±{avg_epoch_loss:.4f} < å†å²æœ€ä½³{best_loss+loss_improve_threshold:.4f}")
        else:
            no_improve_count += 1
            # æ—©åœæ¡ä»¶ï¼šè¿ç»­4æ¬¡æ— æ”¹å–„ï¼Œæˆ–æŸå¤±<0.4ï¼ˆç›®æ ‡è¾¾æˆï¼‰
            if no_improve_count >= 4 or avg_epoch_loss < 0.4:
                if avg_epoch_loss < 0.4:
                    print(f"\nâœ… æŸå¤±è¾¾åˆ°ç›®æ ‡å€¼ï¼ˆ{avg_epoch_loss:.4f}<0.4ï¼‰ï¼ŒåŸè¯—æ‹Ÿåˆå……åˆ†ï¼Œåœæ­¢è®­ç»ƒ")
                else:
                    print(f"\nâœ… æŸå¤±è¿ç»­4æ¬¡æ— æ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ")
                break
    
    # 8. ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆç¡®ä¿åŸè¯—åŒ¹é…æ•ˆæœï¼‰
    if best_model_state is not None:
        torch.save(best_model_state, "exact_match_poem_model.pth")
        print(f"\nğŸ“ åŸè¯—ç²¾å‡†åŒ¹é…æ¨¡å‹å·²ä¿å­˜ï¼šexact_match_poem_model.pthï¼ˆæœ€ä½³æŸå¤±ï¼š{best_loss:.4f}ï¼‰")
    else:
        torch.save(model.state_dict(), "exact_match_poem_model.pth")
        print(f"\nğŸ“ åŸè¯—ç²¾å‡†åŒ¹é…æ¨¡å‹å·²ä¿å­˜ï¼šexact_match_poem_model.pthï¼ˆå½“å‰æŸå¤±ï¼š{avg_epoch_loss:.4f}ï¼‰")
    
    # 9. ä¿å­˜æŸå¤±æ›²çº¿
    with open("exact_match_losses.pkl", "wb") as f:
        pickle.dump(train_losses, f)
    
    # 10. ç»˜åˆ¶æŸå¤±æ›²çº¿
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(10, 4))
        plt.plot(train_losses, color="#E74C3C", linewidth=2, marker="o", markersize=4)
        plt.axhline(y=0.4, color="#3498DB", linestyle="--", label="ç›®æ ‡æŸå¤±ï¼ˆ0.4ï¼‰")
        plt.title("åŸè¯—ç²¾å‡†åŒ¹é…æ¨¡å‹è®­ç»ƒæŸå¤±æ›²çº¿ï¼ˆ20åˆ†é’Ÿé™æ—¶ï¼‰", fontsize=12)
        plt.xlabel("è®­ç»ƒè½®æ¬¡ï¼ˆEpochï¼‰", fontsize=10)
        plt.ylabel("å¹³å‡æŸå¤±å€¼", fontsize=10)
        plt.legend()
        plt.grid(alpha=0.3, linestyle="--")
        plt.savefig("exact_match_loss_curve.png", dpi=100, bbox_inches="tight")
        print(f"ğŸ“Š æŸå¤±æ›²çº¿å·²ä¿å­˜ï¼šexact_match_loss_curve.png")
    except ImportError:
        print("âš ï¸  æœªå®‰è£…matplotlibï¼Œè·³è¿‡æŸå¤±æ›²çº¿ç»˜åˆ¶")
    
    return model, vocab, train_losses

if __name__ == "__main__":
    model, vocab, losses = train_exact_match_model(max_train_time=1200)  # 20åˆ†é’Ÿè®­ç»ƒ
    print("\nğŸ‰ åŸè¯—ç²¾å‡†åŒ¹é…æ¨¡å‹è®­ç»ƒå®Œæˆï¼å¯è¿è¡Œ inference.py ä½“éªŒè¯—è¯ç”Ÿæˆã€‚")
