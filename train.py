import torch
import torch.nn as nn
import torch.optim as optim
import time
import numpy as np
import pickle
from tqdm import tqdm
from dataset import create_enhanced_poetry_dataset, get_data_loader

# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"è®­ç»ƒè®¾å¤‡ï¼š{device}ï¼ˆGPUåŠ é€Ÿå·²å¯ç”¨ï¼‰" if torch.cuda.is_available() else "è®­ç»ƒè®¾å¤‡ï¼šCPUï¼ˆå»ºè®®ä½¿ç”¨GPUæå‡é€Ÿåº¦ï¼‰")

class EnhancedQwen2PoetryModel(nn.Module):
    """å¢å¼ºç‰ˆQwen2è¯—è¯æ¨¡å‹ï¼šæå‡å®¹é‡ä¸è¡¨è¾¾èƒ½åŠ›"""
    def __init__(self, vocab_size, embed_dim=256, num_heads=4, num_layers=4, max_seq_len=64):
        super().__init__()
        self.embed_dim = embed_dim        # æå‡åµŒå…¥ç»´åº¦ï¼ˆ192â†’256ï¼‰
        self.max_seq_len = max_seq_len    # æœ€å¤§åºåˆ—é•¿åº¦
        self.vocab_size = vocab_size      # è¯æ±‡è¡¨å¤§å°
        
        # 1. è¯åµŒå…¥å±‚ï¼ˆå¢åŠ ç»´åº¦ï¼‰
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embed_dim,
            padding_idx=0  # PAD_TOKENçš„ID=0
        )
        
        # 2. æ”¹è¿›çš„ä½ç½®ç¼–ç ï¼ˆæ›´é€‚åˆè¯—è¯ï¼‰
        self.pos_encoding = self._create_positional_encoding(embed_dim, max_seq_len)
        self.pos_encoding.requires_grad = False
        
        # 3. å¢åŠ Transformerå±‚æ•°ï¼ˆ3â†’4ï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,          # å¢åŠ æ³¨æ„åŠ›å¤´æ•°ï¼ˆ3â†’4ï¼‰
            dim_feedforward=1024,     # å¢å¤§å‰é¦ˆç½‘ç»œï¼ˆ768â†’1024ï¼‰
            dropout=0.1,              # é™ä½dropoutï¼ˆ0.15â†’0.1ï¼‰
            layer_norm_eps=1e-5,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=num_layers     # å¢åŠ å±‚æ•°ï¼ˆ3â†’4ï¼‰
        )
        
        # 4. è¾“å‡ºå±‚ï¼ˆå¢åŠ ä¸­é—´å±‚æå‡è¡¨è¾¾ï¼‰
        self.final_norm = nn.LayerNorm(embed_dim)
        self.output_proj = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.GELU(),
            nn.Linear(embed_dim, vocab_size)
        )

    def _create_positional_encoding(self, embed_dim, max_seq_len):
        """æ”¹è¿›çš„ä½ç½®ç¼–ç ï¼Œæ›´é€‚åˆçŸ­æ–‡æœ¬è¯—è¯"""
        position = torch.arange(max_seq_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float32) * (-np.log(10000.0) / embed_dim))
        pos_encoding = torch.zeros(max_seq_len, embed_dim, dtype=torch.float32)
        
        # è°ƒæ•´é¢‘ç‡å‚æ•°ï¼Œæ›´é€‚åˆè¯—è¯çš„çŸ­åºåˆ—ç‰¹æ€§
        pos_encoding[:, 0::2] = torch.sin(position * div_term * 0.5)
        pos_encoding[:, 1::2] = torch.cos(position * div_term * 0.5)
        pos_encoding = pos_encoding.unsqueeze(0)
        return pos_encoding

    def forward(self, x):
        """å‰å‘ä¼ æ’­ï¼šå¢å¼ºç‰¹å¾æå–èƒ½åŠ›"""
        batch_size, seq_len = x.shape
        
        # 1. è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        x_embed = self.embedding(x)
        x_embed = x_embed * np.sqrt(self.embed_dim)
        pos_encoding = self.pos_encoding[:, :seq_len, :].to(x.device)
        x_embed = x_embed + pos_encoding
        
        # 2. Transformer Encoderå¤„ç†
        encoder_out = self.transformer_encoder(x_embed)
        
        # 3. è¾“å‡ºå±‚
        encoder_out = self.final_norm(encoder_out)
        logits = self.output_proj(encoder_out)
        
        return logits

def train_model(max_train_time=600):  # å»¶é•¿è®­ç»ƒæ—¶é—´åˆ°10åˆ†é’Ÿ
    """ä¼˜åŒ–è®­ç»ƒç­–ç•¥ï¼šæå‡æ”¶æ•›è´¨é‡"""
    # 1. åŠ è½½å¢å¼ºç‰ˆæ•°æ®é›†ï¼ˆ12000æ ·æœ¬ï¼‰
    dataset, vocab = create_enhanced_poetry_dataset(
        total_samples=12000,
        max_length=64
    )
    
    # 2. ä¿å­˜è¯æ±‡è¡¨
    with open("vocab_enhanced.pkl", "wb") as f:
        pickle.dump(vocab, f)
    print(f"è¯æ±‡è¡¨å·²ä¿å­˜ï¼švocab_enhanced.pklï¼ˆå¤§å°ï¼š{len(vocab)}ï¼‰")
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 64 if torch.cuda.is_available() else 32
    dataloader = get_data_loader(
        dataset,
        batch_size=batch_size,
        shuffle=True
    )
    print(f"æ•°æ®åŠ è½½å™¨é…ç½®ï¼šæ‰¹æ¬¡å¤§å°={batch_size}ï¼Œæ€»æ‰¹æ¬¡æ•°={len(dataloader)}")
    
    # 4. åˆå§‹åŒ–å¢å¼ºç‰ˆæ¨¡å‹
    model = EnhancedQwen2PoetryModel(
        vocab_size=len(vocab),
        embed_dim=256,
        num_heads=4,
        num_layers=4,
        max_seq_len=64
    ).to(device)
    print("æ¨¡å‹ç»“æ„ï¼š4å±‚Transformer Encoder + 256ç»´åµŒå…¥ï¼ˆå¢å¼ºç‰ˆï¼‰")
    
    # 5. ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ï¼ˆæ›´ç²¾ç»†çš„å‚æ•°ï¼‰
    optimizer = optim.AdamW(
        model.parameters(),
        lr=5e-4,                # è°ƒæ•´å­¦ä¹ ç‡ï¼ˆ6e-4â†’5e-4ï¼Œæ›´ç¨³å®šï¼‰
        weight_decay=1e-5,      # é™ä½æƒé‡è¡°å‡
        betas=(0.9, 0.98)
    )
    
    # å¢åŠ æœ€å¤§epochåˆ°30
    num_epochs = 30
    total_training_steps = len(dataloader) * num_epochs
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=5e-4,
        total_steps=total_training_steps,
        pct_start=0.15,  # å¢åŠ çƒ­èº«æ¯”ä¾‹
        anneal_strategy="cos",  # ä½™å¼¦é€€ç«ï¼Œæ›´ç¨³å®š
        final_div_factor=100
    )
    
    # æ”¹è¿›æŸå¤±å‡½æ•°ï¼ˆé™ä½æ ‡ç­¾å¹³æ»‘ï¼‰
    criterion = nn.CrossEntropyLoss(
        ignore_index=0,
        label_smoothing=0.05  # ä»0.1â†’0.05ï¼Œæ›´å…³æ³¨æ­£ç¡®æ ‡ç­¾
    )
    
    # 6. è®­ç»ƒå‚æ•°ï¼ˆæ›´ä¸¥æ ¼çš„æ—©åœï¼‰
    train_losses = []
    best_loss = float("inf")
    no_improve_count = 0
    start_time = time.time()
    patience = 3  # æ¢å¤åˆ°3æ¬¡ï¼Œå…è®¸æ›´å¤šè¿­ä»£
    
    # 7. å¼€å§‹è®­ç»ƒ
    print(f"\nè®­ç»ƒå¯åŠ¨ï¼ˆé™æ—¶{max_train_time//60}åˆ†é’Ÿï¼‰...")
    model.train()
    for epoch in range(num_epochs):
        elapsed_time = time.time() - start_time
        if elapsed_time >= max_train_time:
            print(f"\nâœ… è¾¾åˆ°è®­ç»ƒæ—¶é—´é™åˆ¶ï¼ˆ{max_train_time}ç§’ï¼‰ï¼Œåœæ­¢è®­ç»ƒ")
            break
        
        epoch_loss = 0.0
        progress_bar = tqdm(
            dataloader,
            desc=f"Epoch {epoch+1:2d}/{num_epochs} | è€—æ—¶ï¼š{elapsed_time:.0f}ç§’"
        )
        
        for batch_idx, (inputs, labels) in enumerate(progress_bar):
            inputs = inputs.to(device, non_blocking=torch.cuda.is_available())
            labels = labels.to(device, non_blocking=torch.cuda.is_available())
            
            optimizer.zero_grad()
            logits = model(inputs)
            loss = criterion(
                logits.reshape(-1, logits.size(-1)),
                labels.view(-1)
            )
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            
            epoch_loss += loss.item()
            avg_batch_loss = epoch_loss / (batch_idx + 1)
            progress_bar.set_postfix({
                "batch_loss": f"{loss.item():.4f}",
                "avg_loss": f"{avg_batch_loss:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.6f}"
            })
        
        avg_epoch_loss = epoch_loss / len(dataloader)
        train_losses.append(avg_epoch_loss)
        print(f"ğŸ“Š Epoch {epoch+1:2d} ç»“æŸ | å¹³å‡æŸå¤±ï¼š{avg_epoch_loss:.4f} | è€—æ—¶ï¼š{elapsed_time:.0f}ç§’")
        
        # æ—©åœç­–ç•¥ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        if avg_epoch_loss < best_loss * 0.99:  # è¦æ±‚æŸå¤±é™ä½1%ä»¥ä¸Šæ‰ç®—æ”¹å–„
            best_loss = avg_epoch_loss
            no_improve_count = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), "qwen2_best_poetry_model.pth")
        else:
            no_improve_count += 1
            if no_improve_count >= patience:
                print(f"\nâœ… æŸå¤±è¿ç»­{patience}æ¬¡æ— æ”¹å–„ï¼Œæå‰åœæ­¢")
                break
        
        # ç›®æ ‡æŸå¤±å€¼è°ƒæ•´ä¸ºæ›´ä½ï¼ˆ<0.6ï¼‰
        if avg_epoch_loss < 0.6:
            print(f"\nâœ… æŸå¤±è¾¾åˆ°ç›®æ ‡å€¼ï¼ˆ<0.6ï¼‰ï¼Œæå‰åœæ­¢è®­ç»ƒ")
            break
    
    # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), "qwen2_enhanced_poetry_model.pth")
    print(f"\nğŸ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ï¼šqwen2_enhanced_poetry_model.pth")
    print(f"ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼šqwen2_best_poetry_model.pth")
    
    # 9. ä¿å­˜æŸå¤±æ›²çº¿
    with open("train_losses_enhanced.pkl", "wb") as f:
        pickle.dump(train_losses, f)
    
    # 10. ç»˜åˆ¶æŸå¤±æ›²çº¿
    try:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(10, 4))
        plt.plot(train_losses, color="#2E86AB", linewidth=2, marker="o", markersize=4)
        plt.title("å¢å¼ºç‰ˆè¯—è¯æ¨¡å‹è®­ç»ƒæŸå¤±æ›²çº¿", fontsize=12)
        plt.xlabel("è®­ç»ƒè½®æ¬¡ï¼ˆEpochï¼‰", fontsize=10)
        plt.ylabel("å¹³å‡æŸå¤±å€¼", fontsize=10)
        plt.grid(alpha=0.3, linestyle="--")
        plt.savefig("train_loss_curve_enhanced.png", dpi=100, bbox_inches="tight")
        print(f"ğŸ“Š æŸå¤±æ›²çº¿å·²ä¿å­˜ï¼štrain_loss_curve_enhanced.png")
    except ImportError:
        print("âš ï¸  æœªå®‰è£…matplotlibï¼Œè·³è¿‡æŸå¤±æ›²çº¿ç»˜åˆ¶")
    
    return model, vocab, train_losses

if __name__ == "__main__":
    model, vocab, losses = train_model(max_train_time=600)  # 10åˆ†é’Ÿ=600ç§’
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼å¯è¿è¡Œ inference.py ä½“éªŒè¯—è¯ç”Ÿæˆã€‚")
