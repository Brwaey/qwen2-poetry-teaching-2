# 基于Qwen2的中文诗词生成教学演示项目

## 1. 项目概述

### 1.1 项目目标
本项目是一个专为高中AI课堂设计的轻量级大语言模型教学案例，旨在于45分钟的课堂时间内，通过一个直观、有趣的中文诗词生成任务，帮助学生：
- **理解大语言模型（LLM）的基本工作原理**：学习文本数据如何被处理、输入和输出。
- **掌握核心训练流程**：体验从数据准备、模型训练到推理生成的完整过程。
- **了解Transformer架构**：通过简化的Qwen2模型，初步认识Transformer Encoder在自然语言处理中的应用。

### 1.2 主要功能
- **诗词续写**：用户输入一句诗的开头，模型能够自动生成符合格律和平仄的后续诗句，并添加正确的标点符号。
- **快速训练**：项目经过高度优化，可在普通CPU或入门级GPU上实现5-10分钟的快速训练，适应课堂教学节奏。
- **效果可控**：通过“原诗精准匹配”策略，确保模型生成的内容100%忠于预设的经典唐诗，保证了教学演示的准确性和稳定性。

### 1.3 适用场景
- **高中AI教学**：作为《人工智能》课程中关于自然语言处理或大语言模型章节的配套实践案例。
- **编程兴趣小组**：用于激发学生对AI、深度学习和Python编程的兴趣。
- **技术普及讲座**：作为一个简单直观的演示，向非专业人士展示大语言模型的能力。

---

## 2. 环境准备

### 2.1 硬件要求
项目对硬件配置要求较低，支持主流的CPU和GPU环境。

| 设备类型 | 最低配置                | 推荐配置                | 训练时间预估            |
|----------|-------------------------|-------------------------|-------------------------|
| CPU      | Intel i5-8代/AMD R5-3500U | Intel i7-10代/AMD R7-5800H | 8-10分钟                |
| GPU      | NVIDIA GTX 1060（6G）   | NVIDIA RTX 3050（4G）   | 3-5分钟                 |
| 内存     | 8GB                     | 16GB                    | -                       |
| 硬盘     | 100MB空闲空间           | 200MB空闲空间           | -                       |

### 2.2 软件依赖
- **操作系统**: Windows 10/11, macOS, or Linux
- **Python版本**: **3.8 ~ 3.10** (推荐使用 **3.9** 以获得最佳兼容性)
- **主要依赖库**:
  - `torch`: 1.8.0+
  - `jieba`: 0.42.1
  - `tqdm`: 4.66.1
  - `numpy`: 1.26.3
  - `matplotlib`: 3.8.2 (可选，用于绘制损失曲线)

---

## 3. 项目结构

项目由核心模块和进阶的容错微调模块组成。

```
qwen2-poetry-teaching/
├── 📄 README.md                    # 本文档，提供完整的项目说明
├── 🐍 dataset.py                   # 核心模块1：数据集构建
├── 🐍 train.py                     # 核心模块2：模型定义与训练
├── 🐍 inference.py                 # 核心模块3：诗词生成与交互
├── 📄 requirements.txt             # Python依赖库清单
│
├── 🐍 dataset_fault_tolerance.py   # (进阶) 容错数据集构建
├── 🐍 train_fault_tolerance.py     # (进阶) 容错模型微调
├── 🐍 inference_fault_tolerance.py # (进阶) 容错推理演示
│
├── 🖼️ exact_match_loss_curve.png   # (生成) 基础模型损失曲线
├── 🧠 exact_match_poem_model.pth   # (生成) 基础模型权重
└── 📖 exact_match_vocab.pkl        # (生成) 基础模型词汇表
```

### 关键目录/文件说明
- **`dataset.py`**: 负责将内置的经典唐诗处理成模型可以学习的`输入-目标`样本对。
- **`train.py`**: 定义了简化的Qwen2模型结构，并包含完整的训练逻辑，如优化器、损失函数和早停策略。
- **`inference.py`**: 用于加载训练好的模型，提供一个交互式命令行界面，让用户输入诗句并查看生成结果。
- **`requirements.txt`**: 包含了所有必需的Python库，方便一键安装。
- **生成文件**: 运行`train.py`后，会生成模型（`.pth`）、词汇表（`.pkl`）和损失曲线（`.png`）等文件。

---

## 4. 核心功能详解

### 4.1 `dataset.py`：原诗精准匹配数据集
该文件的核心是`create_exact_match_dataset`函数，其实现逻辑如下：
1.  **内置原诗库**：内置了多首经典唐诗（如《静夜思》、《春望》），每首诗都附带了标准句式（如“七言绝句”）和所有可能的开头。
2.  **样本生成**：通过从原诗库中随机选取“开头-后续”对，生成20,000个训练样本。这种方法确保了每个样本的输入和目标都严格来自同一首诗，避免了模型学习到混乱的、跨诗歌的组合。
3.  **词汇表构建**：创建一个`ExactMatchVocab`词汇表，该词汇表强制包含所有原诗中的单字，不进行任何低频词过滤。这保证了模型在生成时不会因为词汇不足而产生`<unk>`（未知词）标记。
4.  **数据封装**：最终将处理好的数据封装成PyTorch的`Dataset`和`DataLoader`对象，供`train.py`使用。

### 4.2 `train.py`：模型定义与训练
该文件负责模型的构建和训练，核心逻辑包括：
1.  **模型定义 (`ExactMatchModel`)**：
    -   **嵌入层 (`nn.Embedding`)**: 将每个汉字（Token）映射到一个256维的向量。
    -   **位置编码**: 采用周期性的正弦/余弦位置编码，帮助模型理解汉字在诗句中的位置信息，这对于学习格律至关重要。
    -   **Transformer Encoder (`nn.TransformerEncoder`)**: 堆叠了3层Transformer编码器。这是模型的核心，负责捕捉诗句内部的语义和结构关系。我们特意选择了较少的层数和极低的Dropout率（0.05），以强化模型对原诗的“记忆”能力。
    -   **输出层 (`nn.Linear`)**: 将Transformer的输出映射回词汇表大小，得到每个字在下一个位置出现的概率。
2.  **训练逻辑 (`train_exact_match_model`)**：
    -   **优化器与调度器**: 使用`AdamW`优化器和`CosineAnnealingLR`学习率调度器，以实现稳定、高效的收敛。
    -   **损失函数**: 采用标准的`CrossEntropyLoss`，并且**禁用了标签平滑**，目的是让模型以100%的置信度学习原诗的每一个字。
    -   **早停策略**: 训练过程中会实时监控损失值（`avg_loss`）。当损失值低于0.4或连续4个轮次没有显著下降时，训练会自动停止，以防止过拟合，并节省时间。
    -   **模型保存**: 训练结束后，会将损失最低的模型权重保存为`exact_match_poem_model.pth`。

### 4.3 `inference.py`：诗词生成与交互
该文件是最终成果的展示平台，其工作流程如下：
1.  **加载资源**：首先加载`train.py`生成的模型文件（`.pth`）、词汇表（`.pkl`）和原诗库（`original_poems.pkl`）。
2.  **匹配原诗**：当用户输入一个开头时，`match_input_to_poem`函数会去原诗库中查找最匹配的诗歌，并获取其标准句式信息。
3.  **逐字生成 (`generate_exact_poem`)**：
    -   将用户输入编码为ID序列。
    -   进入一个循环，每次都将当前的ID序列输入模型，预测下一个最可能的汉字ID。
    -   **关键优化**：在预测时，会强制模型只能选择**隶属于所匹配原诗的汉字**，从而杜绝了任何“即兴创作”或“跨诗串烧”的可能，确保了100%的准确性。
    -   当生成的长度达到原诗长度或模型输出`EOS`（结束）标记时，循环停止。
4.  **添加标点**：`add_standard_punctuation`函数会根据匹配到的原诗句式（如“七言绝句”），在生成文本的正确位置（如第7、14、21个字后）添加“，”或“。”。
5.  **交互界面**：`interactive_exact_demo`函数提供了一个用户友好的命令行界面，引导用户输入、显示结果，并处理`help`、`exit`等命令。

---

## 5. 使用指南

### 5.1 步骤一：安装依赖
首先，确保您的Python环境符合要求（3.8-3.10）。然后，在项目根目录下打开终端，运行以下命令安装所有依赖：

**对于CPU环境（通用）：**
```bash
# (推荐) 创建并激活conda虚拟环境
conda create -n qwen2-poetry python=3.9
conda activate qwen2-poetry

# 安装PyTorch的CPU版本
pip install torch==1.8.0+cpu -f https://download.pytorch.org/whl/torch_stable.html

# 安装其他依赖
pip install -r requirements.txt
```

**对于GPU环境（需要NVIDIA显卡）：**
```bash
# 确保已安装与显卡驱动匹配的CUDA
# 示例：安装支持CUDA 11.1的PyTorch
pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html

# 安装其他依赖
pip install -r requirements.txt
```

### 5.2 步骤二：启动训练
在项目根目录的终端中，运行以下命令开始训练：
```bash
python train.py
```
**预期效果说明：**
- 终端会首先显示`训练设备：CPU`或`训练设备：cuda`。
- 接着，会看到数据集创建的进度条，提示“生成原诗精准匹配样本”。
- 然后，训练正式开始，会逐轮（Epoch）显示训练进度、耗时和平均损失（`avg_loss`）。
- **成功的标志**是`avg_loss`从一个较高的值（如4.0+）逐渐下降到**0.4以下**。
- 训练结束后，会提示“原诗精准匹配模型已保存”和“损失曲线已保存”。

### 5.3 步骤三：运行生成程序
训练完成后，运行以下命令启动交互式生成程序：
```bash
python inference.py
```
**预期效果说明：**
- 终端会显示一个欢迎界面，列出所有支持续写的原诗。
- 提示“演示启动成功！请输入上述原诗的开头：”后，即可开始体验。
- **示例输入**：`床前明月光`
- **预期输出**：
  ```
  🎯 原诗精准匹配结果：
  《静夜思》
  床前明月光，疑是地上霜。举头望明月，低头思故乡。
  ```
- 输入`exit`可退出程序。

---

## 6. 常见问题 (FAQ)

| 问题现象                                  | 可能原因                                  | 解决方案与排查方法                                  |
|-------------------------------------------|-------------------------------------------|-----------------------------------------------------|
| `ModuleNotFoundError: No module named 'torch'` | 未正确安装PyTorch或未激活虚拟环境。 | 1. 确认已按照**步骤5.1**安装依赖。<br>2. 如果使用了conda环境，请确保已运行`conda activate qwen2-poetry`。 |
| 训练时GPU未被使用（设备显示为CPU）        | 1. PyTorch的GPU版本未正确安装。<br>2. CUDA版本与PyTorch不匹配。 | 1. 卸载当前torch：`pip uninstall torch`。<br>2. 严格按照**步骤5.1**中的GPU环境指南重新安装。<br>3. 运行`python -c "import torch; print(torch.cuda.is_available())"`检查，应输出`True`。 |
| 训练损失（`avg_loss`）降不下去，一直很高 | 1. 数据集有问题。<br>2. 模型参数设置不当。 | 1. 删除项目目录下的所有`.pkl`文件，重新运行`train.py`以重建数据集。<br>2. 检查`train.py`中的学习率（`lr`）是否过高，可尝试从`3e-4`调低至`1e-4`。 |
| `FileNotFoundError` (找不到模型或词汇表文件) | 未成功完成训练，或文件被移动/删除。 | 1. 检查项目目录下是否存在`exact_match_poem_model.pth`和`exact_match_vocab.pkl`文件。<br>2. 如果不存在，请重新运行`train.py`完成一次完整的训练。 |
| 生成结果不完整或重复 | 1. 模型训练不充分（损失 > 0.5）。<br>2. `inference.py`中的生成长度限制过短。 | 1. 重新训练模型，确保最终损失低于0.4。<br>2. 在`inference.py`的`generate_exact_poem`函数中，适当增大`max_length`参数的值。 |

---

## 7. 注意事项与最佳实践

1.  **首次运行**: 第一次运行时，请务必先完整地运行`train.py`，再运行`inference.py`。
2.  **CPU训练时间**: 在纯CPU环境下，训练耗时约8-10分钟是正常现象。建议在课前提前完成训练，课堂上直接进行`inference.py`的演示。
3.  **修改诗歌库**: 如果您想添加或修改诗歌，可以直接编辑`dataset.py`文件中的`original_poems`列表。修改后，需要删除旧的`.pkl`文件并重新训练。
4.  **版本兼容性**: 本项目代码在`torch 1.8.0`下经过充分测试。若使用更高版本的PyTorch（如2.0+），可能会出现一些`UserWarning`，但通常不影响核心功能。
5.  **教学建议**:
    -   在讲解`dataset.py`时，可以重点突出“数据增强”（生成所有可能的开头）和“词汇表”的概念。
    -   在讲解`train.py`时，可以结合损失曲线图，向学生解释“模型收敛”和“早停”的意义。
    -   在演示`inference.py`时，可以引导学生思考“模型是如何做到100%准确的”，从而引出对代码中“强制选择原诗汉字”这一关键优化的讨论。

---

## 8. 进阶功能：容错输入微调

在基础模型之上，我们提供了一个进阶的微调功能，旨在提升模型的鲁棒性，使其能够处理含有错别字或不完整的用户输入。

### 8.1 功能目标
使模型具备一定的“纠错”能力。例如，当用户输入`床前明月广`（错字）或`白日依山`（不完整）时，模型仍能大概率匹配到正确的原诗并生成完整内容。

### 8.2 实现原理
1.  **构造容错数据集**：通过`dataset_fault_tolerance.py`，我们人工构造了一批“错误输入-正确输出”的数据对。例如，`("国破山河再", "国破山河在，城春草木深。")`。
2.  **加载预训练模型**：`train_fault_tolerance.py`会首先加载已经训练好的基础模型`exact_match_poem_model.pth`。
3.  **进行微调（Fine-tuning）**：使用容错数据集对加载的模型进行几个轮次的额外训练。这个过程会微调模型参数，使其在保持原有能力的同时，学会将相似的错误输入映射到正确的输出上。

### 8.3 核心文件说明
- **`dataset_fault_tolerance.py`**: 包含一个`create_fault_tolerant_dataset`函数，专门用于生成包含错字、漏字的“脏”数据样本。
- **`train_fault_tolerance.py`**: 负责加载基础模型和容错数据集，执行微调过程，并保存微调后的新模型`fault_tolerance_poem_model.pth`。
- **`inference_fault_tolerance.py`**: 用于加载和演示微调后模型的效果。其界面与`inference.py`类似，但使用的是具备容错能力的新模型。

### 8.4 使用步骤

**前提：** 必须先按照**第5章**的指引，成功训练并生成`exact_match_poem_model.pth`基础模型。

**步骤一：启动容错微调**
```bash
python train_fault_tolerance.py
```
- **预期效果**：训练过程非常快（通常1分钟内完成），因为它只在少量数据上进行微调。训练结束后，会生成`fault_tolerance_poem_model.pth`。

**步骤二：运行容错推理程序**
```bash
python inference_fault_tolerance.py
```
- **预期效果**：
  - 启动后，会加载`fault_tolerance_poem_model.pth`。
  - 您可以尝试输入一些错误的诗句开头。
  - **示例输入**：`国破山河再`
  - **预期输出**：
    ```
    🎯 容错匹配结果：
    《春望》
    国破山河在，城春草木深。感时花溅泪，恨别鸟惊心。
    ```